{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Buildingchatbot_Industryproject to ED.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1s7N3OjhpykI0ya8Ni4iJmJ9mrWHIz0--","authorship_tag":"ABX9TyPS3/5uL7tMR04gV0bc/4bz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2G1Yqk82neZ9","colab_type":"code","outputId":"ed557eb9-c28c-4311-d7ae-02e25a490113","executionInfo":{"status":"ok","timestamp":1587311532073,"user_tz":-330,"elapsed":2499812,"user":{"displayName":"laxmi sudha chavali","photoUrl":"https://lh6.googleusercontent.com/--HkVio7EZ-c/AAAAAAAAAAI/AAAAAAAAAd8/hGPdSwiXFlg/s64/photo.jpg","userId":"11752698593148727203"}},"colab":{"base_uri":"https://localhost:8080/","height":994}},"source":["from keras.models import Model\n","from keras.layers.recurrent import LSTM\n","from keras.layers import Dense, Input, Embedding\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import ModelCheckpoint\n","from collections import Counter\n","import nltk\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from keras.utils import plot_model\n","from IPython.display import Image\n","\n","import keras\n","import nltk\n","nltk.download('punkt')\n","import numpy\n","import sklearn\n","!pip install -q pydot\n","\n","\n","#Load  the glove twitter zip file\n","#! curl -O http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip \n","\n","\n","RAND_STATE=np.random.seed(42)\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 10 \n","GLOVE_EMBEDDING_SIZE = 100\n","HIDDEN_UNITS = 256\n","MAX_INPUT_SEQ_LENGTH = 40\n","MAX_TARGET_SEQ_LENGTH = 40\n","MAX_VOCAB_SIZE = 10000\n","\n","\n","\n","DATA_SET_NAME = '/content/drive/My Drive/Colab Notebooks/cornell'\n","DATA_PATH = '/content/drive/My Drive/Colab Notebooks/cornell/movie_lines_cleaned.txt'\n","GLOVE_MODEL = \"/content/drive/My Drive/Colab Notebooks/cornell/glovedataset/glove.twitter.27B.100d.txt\"\n","WHITELIST = 'abcdefghijklmnopqrstuvwxyz1234567890?.,'\n","WEIGHT_FILE_PATH =  DATA_SET_NAME + '/word-glove-weights.h5'\n","\n","\n","'''Check if the characters in the words are whitelisted'''\n","def in_white_list(_word):\n","  for char in _word:\n","        if char in WHITELIST:\n","            return True\n","        else:\n","          return False\n","\n","#Prepare input data with embedding\n","def load_glove_vector():\n","    _word2embedding = {}\n","    file = open(GLOVE_MODEL, mode='rt', encoding='utf8')\n","    for line in file:\n","        words = line.strip().split()\n","        word = words[0]\n","        embeds = np.array(words[1:], dtype=np.float32)\n","        _word2embedding[word] = embeds\n","    file.close()\n","    return _word2embedding\n","\n","word2embedding = load_glove_vector()     \n","print(WHITELIST)          \n","\n","print(str(len(word2embedding)))\n","\n","assert len(word2embedding.keys())==1193514\n","for key in word2embedding.keys():\n","    try:\n","        assert len(word2embedding[key])==100\n","        #print(\"A\")\n","    except AssertionError:\n","        print (key,len(word2embedding[key]))\n","        #print(\"B\") \n","\n","input_counter = Counter()\n","target_counter = Counter()\n","\n","\n","\n","# read the data\n","with open(DATA_PATH, 'r', encoding=\"utf8\") as f:\n","    df = f.read()\n","print(df[:100])\n","\n","rows = df.split('\\n')\n","#lines = open(DATA_PATH, 'rt', encoding='utf8').read().split('\\n')\n","lines = [row.split(' +++$+++ ')[-1] for row in rows]\n","print(df[:100])\n","input_texts = []\n","target_texts = []\n","prev_words = []\n","\n","\n","\n","for line in lines:\n","    next_words = [w.lower() for w in nltk.word_tokenize(line)]\n","    if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n","        next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n","\n","    if len(prev_words) > 0:\n","        input_texts.append(prev_words)\n","        for w in prev_words:\n","            input_counter[w] += 1\n","        target_words = next_words[:]\n","        target_words.insert(0, 'start')\n","        target_words.append('end')\n","        for w in target_words:\n","            target_counter[w] += 1\n","        target_texts.append(target_words)\n","    prev_words = next_words\n","\n","for idx, (input_words, target_words) in enumerate(zip(input_texts, target_texts)):\n","    if idx > 10:\n","        break\n","    print([input_words, target_words])   \n","\n","\n","#'''create a target word to id dictionary called target_word2idx.\n","#'''create a target to id dictionary called input_word2idx . Approx ~1 line'''\n","\n","# encode the data\n","input_word2idx = dict()\n","target_word2idx = dict()\n","\n","\n","if 'unk' not in target_word2idx:\n","    target_word2idx['unk'] = 0\n","\n","\n","input_word2idx['PAD'] = 0\n","input_word2idx['UNK'] = 1\n","target_word2idx['UNK'] = 0\n","\n","for idx, word in enumerate(input_counter.most_common(MAX_VOCAB_SIZE)):\n","    input_word2idx[word[0]] = idx + 2\n","for idx, word in enumerate(target_counter.most_common(MAX_VOCAB_SIZE)):\n","    target_word2idx[word[0]] = idx + 1\n","\n","\n","input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n","target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n","\n","num_encoder_tokens = len(input_idx2word)\n","num_decoder_tokens = len(target_idx2word)\n","\n","np.save( DATA_SET_NAME + '/word-glove-target-word2idx.npy', target_word2idx)\n","np.save( DATA_SET_NAME + '/word-glove-target-idx2word.npy', input_word2idx)\n","\n","\n","print(num_encoder_tokens)\n","\n","\n","encoder_input_data = []\n","\n","encoder_max_seq_length = 0\n","decoder_max_seq_length = 0\n","\n","for input_words, target_words in zip(input_texts, target_texts):\n","    encoder_input_wids = []\n","    for w in input_words:\n","        w2idx = 1\n","        if w in input_word2idx:\n","            w2idx = input_word2idx[w]\n","        encoder_input_wids.append(w2idx)\n","\n","    encoder_input_data.append(encoder_input_wids)\n","    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n","    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)\n","\n","\n","context = dict()\n","context['num_encoder_tokens'] = num_encoder_tokens\n","context['num_decoder_tokens'] = num_decoder_tokens\n","context['encoder_max_seq_length'] = encoder_max_seq_length\n","context['decoder_max_seq_length'] = decoder_max_seq_length\n","\n","np.save( DATA_SET_NAME + '/word-context.npy', context)\n","\n","for input_text,input_text_embed in zip (input_texts,range(len(encoder_input_data))):\n","    assert (len(input_text)==len(encoder_input_data[input_text_embed]))\n","\n","# custom function to generate batches\n","\n","def generate_batch(input_data, output_text_data):\n","    num_batches = len(input_data) // BATCH_SIZE\n","    while True:\n","        for batchIdx in range(0, num_batches):\n","            start = batchIdx * BATCH_SIZE\n","            end = (batchIdx + 1) * BATCH_SIZE\n","            encoder_input_data_batch = pad_sequences(input_data[start:end], encoder_max_seq_length)\n","            decoder_target_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n","            decoder_input_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n","            for lineIdx, target_words in enumerate(output_text_data[start:end]):\n","                for idx, w in enumerate(target_words):\n","                    w2idx = 0\n","                    if w in target_word2idx:\n","                        w2idx = target_word2idx[w]\n","                    decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n","                    if idx > 0:\n","                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n","            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n","\n","\n","# Compiling and training\n","\n","encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n","encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=HIDDEN_UNITS,\n","                              input_length=encoder_max_seq_length, name='encoder_embedding')\n","encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n","encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n","encoder_states = [encoder_state_h, encoder_state_c]\n","\n","decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n","decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n","decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n","                                                                 initial_state=encoder_states)\n","decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.summary()\n","\n","#Model Architechture\n","\n","plot_model(model, to_file='model.png')\n","\n","Image(filename='model.png',height=400,width=400)\n","#print(Image)\n","\n","\n","model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n","\n","json = model.to_json()\n","open(DATA_SET_NAME + 'word-architecture.json', 'w').write(json)\n","\n","#Create Train and Test data\n","X_train, X_test, y_train, y_test = train_test_split(encoder_input_data, target_texts, test_size=0.2, random_state=42)\n","\n","train_gen = generate_batch(X_train, y_train)\n","test_gen = generate_batch(X_test, y_test)\n","\n","train_num_batches = len(X_train) // BATCH_SIZE\n","test_num_batches = len(X_test) // BATCH_SIZE\n","\n","\n","model.fit_generator(generator=train_gen,\n","                    steps_per_epoch=train_num_batches,\n","                    epochs=NUM_EPOCHS,\n","                    verbose=1,\n","                    validation_data=test_gen,\n","                    validation_steps=test_num_batches)\n","\n","model.save_weights(WEIGHT_FILE_PATH)\n","\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","abcdefghijklmnopqrstuvwxyz1234567890?.,\n","1193514\n","-0.32053 99\n","They do not!\n","They do to!\n","I hope so.\n","She okay?\n","Let's go.\n","Wow\n","Okay -- you're gonna need to learn how t\n","They do not!\n","They do to!\n","I hope so.\n","She okay?\n","Let's go.\n","Wow\n","Okay -- you're gonna need to learn how t\n","[['they', 'do', 'not', '!'], ['start', 'they', 'do', 'to', '!', 'end']]\n","[['they', 'do', 'to', '!'], ['start', 'i', 'hope', 'so', '.', 'end']]\n","[['i', 'hope', 'so', '.'], ['start', 'she', 'okay', '?', 'end']]\n","[['she', 'okay', '?'], ['start', 'let', \"'s\", 'go', '.', 'end']]\n","[['let', \"'s\", 'go', '.'], ['start', 'wow', 'end']]\n","[['wow'], ['start', 'okay', '--', 'you', \"'re\", 'gon', 'na', 'need', 'to', 'learn', 'how', 'to', 'lie', '.', 'end']]\n","[['okay', '--', 'you', \"'re\", 'gon', 'na', 'need', 'to', 'learn', 'how', 'to', 'lie', '.'], ['start', 'no', 'end']]\n","[['no'], ['start', 'i', \"'m\", 'kidding', '.', 'you', 'know', 'how', 'sometimes', 'you', 'just', 'become', 'this', '``', 'persona', \"''\", '?', 'and', 'you', 'do', \"n't\", 'know', 'how', 'to', 'quit', '?', 'end']]\n","[['i', \"'m\", 'kidding', '.', 'you', 'know', 'how', 'sometimes', 'you', 'just', 'become', 'this', '``', 'persona', \"''\", '?', 'and', 'you', 'do', \"n't\", 'know', 'how', 'to', 'quit', '?'], ['start', 'like', 'my', 'fear', 'of', 'wearing', 'pastels', '?', 'end']]\n","[['like', 'my', 'fear', 'of', 'wearing', 'pastels', '?'], ['start', 'the', '``', 'real', 'you', \"''\", '.', 'end']]\n","[['the', '``', 'real', 'you', \"''\", '.'], ['start', 'what', 'good', 'stuff', '?', 'end']]\n","10002\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","encoder_inputs (InputLayer)     (None, None)         0                                            \n","__________________________________________________________________________________________________\n","encoder_embedding (Embedding)   (None, 40, 256)      2560512     encoder_inputs[0][0]             \n","__________________________________________________________________________________________________\n","decoder_inputs (InputLayer)     (None, None, 10001)  0                                            \n","__________________________________________________________________________________________________\n","encoder_lstm (LSTM)             [(None, 256), (None, 525312      encoder_embedding[0][0]          \n","__________________________________________________________________________________________________\n","decoder_lstm (LSTM)             [(None, None, 256),  10504192    decoder_inputs[0][0]             \n","                                                                 encoder_lstm[0][1]               \n","                                                                 encoder_lstm[0][2]               \n","__________________________________________________________________________________________________\n","decoder_dense (Dense)           (None, None, 10001)  2570257     decoder_lstm[0][0]               \n","==================================================================================================\n","Total params: 16,160,273\n","Trainable params: 16,160,273\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1\n","7611/7611 [==============================] - 2403s 316ms/step - loss: 1.4538 - val_loss: 1.6591\n"],"name":"stdout"}]}]}